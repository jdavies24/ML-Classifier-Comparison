import sklearn
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn import preprocessing
import matplotlib.pyplot as plt
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, f1_score, precision_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import learning_curve
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.linear_model import SGDClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

X = total_data.drop(['Outcome'], axis=1)
y = total_data['Outcome']

names = [
    "Nearest_Neighbors", "Linear_SVM", "Polynomial_SVM", "RBF_SVM", "Gaussian_Process",
    "Gradient_Boosting", "Decision_Tree", "Extra_Trees", "Random_Forest", "Neural_Net",
    "AdaBoost", "Naive_Bayes", "QDA", "SGD", "LDA"
]

classifiers = [
    KNeighborsClassifier(3),
    SVC(C=0.01, gamma=1, kernel='linear'),
    SVC(kernel="poly", degree=3, C=0.025),
    SVC(kernel="rbf", C=1, gamma=2),
    GaussianProcessClassifier(1.0 * RBF(1.0)),
    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0),
    DecisionTreeClassifier(max_depth=5),
    ExtraTreesClassifier(n_estimators=10, min_samples_split=2),
    RandomForestClassifier(max_depth=5, n_estimators=100),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(n_estimators=100),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    SGDClassifier(loss="hinge", penalty="l2"),
    LinearDiscriminantAnalysis()
]

random_seeds = [18533, 12345, 98765, 54321, 11111, 99999, 77777, 22222, 66666, 44444]

for seed in random_seeds:
    random.seed(seed)
    np.random.seed(seed)

    time_list = []  # Initialize an empty list for storing time

    scores = []
    recalls = []
    f1_scores = []
    precisions = []
    times = []

    for name, clf in zip(names, classifiers):
        start_time = time.time()
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)
        scores.append(score)
        y_pred = clf.predict(X_test)
        recall = recall_score(y_test, y_pred, average=None)
        recalls.append(recall)
        f1 = f1_score(y_test, y_pred, average=None)
        f1_scores.append(f1)
        precision = precision_score(y_test, y_pred, average=None)
        precisions.append(precision)
        end_time = time.time()
        elapsed_time = end_time - start_time
        time_list.append(elapsed_time)  # Append elapsed time to the list

        # Plot confusion matrix for each classifier
        disp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, cmap=plt.cm.Blues)
        plt.title(name)
        plt.savefig(f"{name}_confusion_matrix_seed_{seed}.png", dpi=1000)
        plt.show()

        # Set up the figure and axes for learning curve
        fig, ax = plt.subplots(figsize=(8, 6))

        # Plot the learning curve
        train_sizes, train_scores, test_scores = learning_curve(clf, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10))
        train_scores_mean = np.mean(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)
        ax.plot(train_sizes, train_scores_mean, label='Training score')
        ax.plot(train_sizes, test_scores_mean, label='Cross-validation score')
        ax.set_xlabel('Training examples')
        ax.set_ylabel('Score')
        ax.legend(loc='best')
        ax.set_title(f'Learning Curve ({name})')
        ax.set_ylim(0, 1.1)

        plt.tight_layout()
        plt.savefig(f"{name}_learning_curve_seed_{seed}.png", dpi=1000)
        plt.show()

    times.extend(time_list)  # Extend the main 'times' list with the elapsed times

    # Plotting time
    ti = pd.DataFrame()
    ti['name'] = names
    ti['time'] = times

    sns.set(style="whitegrid")
    ax = sns.barplot(y="name", x="time", data=ti)
    ax.set_title(f'Time - Random Seed {seed}')
    plt.savefig(f"time_seed_{seed}.png", dpi=1000)
    plt.show()

    # Plotting score
    sc = pd.DataFrame()
    sc['name'] = names
    sc['score'] = scores

    sns.set(style="whitegrid")
    ax = sns.barplot(y="name", x="score", data=sc)
    ax.set_title(f'Score - Random Seed {seed}')
    plt.savefig(f"score_seed_{seed}.png", dpi=1000)
    plt.show()

    # Plotting precision
    pr = pd.DataFrame()
    pr['name'] = names
    pr['precision'] = precisions

    sns.set(style="whitegrid")
    ax = sns.barplot(y="name", x="precision", data=pr)
    ax.set_title(f'Precision - Random Seed {seed}')
    plt.savefig(f"precision_seed_{seed}.png", dpi=1000)
    plt.show()

    # Plotting F1 score
    f1_sc = pd.DataFrame()
    f1_sc['name'] = names
    f1_sc['f1'] = f1_scores

    sns.set(style="whitegrid")
    ax = sns.barplot(y="name", x="f1", data=f1_sc)
    ax.set_title(f'F1 Score - Random Seed {seed}')
    plt.savefig(f"f1_score_seed_{seed}.png", dpi=1000)
    plt.show()

    # Plotting recall
    rec = pd.DataFrame()
    rec['name'] = names
    rec['recall'] = recalls

    sns.set(style="whitegrid")
    ax = sns.barplot(y="name", x="recall", data=rec)
    ax.set_title(f'Recall - Random Seed {seed}')
    plt.savefig(f"recall_seed_{seed}.png", dpi=1000)
    plt.show()
